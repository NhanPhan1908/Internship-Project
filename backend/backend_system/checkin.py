from datetime import datetime
import cv2
import numpy as np
from pymongo import MongoClient
from PIL import Image
import torch
from scipy.spatial.distance import cosine
from facenet_pytorch import MTCNN, InceptionResnetV1
from torchvision import transforms

# üéØ K·∫øt n·ªëi MongoDB
client = MongoClient("mongodb://localhost:27017/")
db = client["attendance"]
collection_employees = db["employees"]  # S·ª≠a l·ªói t√™n collection
collection_checkin = db["checkin"]
collection_checkout = db["checkout"]

# üìå C·∫•u h√¨nh gi·ªù l√†m
CHECKIN_START = 8      # Check-in s·ªõm nh·∫•t
LATE_CHECKIN_LIMIT = 9 # Tr·ªÖ nh·∫•t ƒë∆∞·ª£c ph√©p check-in
CHECKOUT_HOUR = 17     # Check-out s·ªõm nh·∫•t

# üñ• Ki·ªÉm tra GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
mtcnn = MTCNN(keep_all=False, device=device)
facenet = InceptionResnetV1(pretrained="casia-webface").eval().to(device)

# üìå Ti·ªÅn x·ª≠ l√Ω ·∫£nh
def fixed_image_standardization(image_tensor):
    return (image_tensor - 127.5) / 128.0

def trans(img):
    transform = transforms.Compose([
        transforms.Resize((160, 160)),  # Resize ƒë·ªÉ ph√π h·ª£p v·ªõi model
        transforms.ToTensor(),
        fixed_image_standardization
    ])
    return transform(img)

# üîç T·∫£i embeddings t·ª´ database
def load_embeddings_from_db():
    embeddings = []
    names = []

    for record in collection_employees.find({}, {"name": 1, "embedding": 1, "_id": 0}):
        names.append(record["name"])
        embeddings.append(np.array(record["embedding"]))  # Chuy·ªÉn sang NumPy array lu√¥n

    if len(embeddings) == 0:
        return None, None  # Tr·∫£ v·ªÅ None n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu

    return np.array(embeddings), names  # Tr·∫£ v·ªÅ NumPy array

# üïí X·ª≠ l√Ω check-in/check-out
def save_attendance(name, confidence):
    now = datetime.now()
    hour, minute = now.hour, now.minute
    timestamp = now.strftime("%Y-%m-%d %H:%M:%S")

    if hour < LATE_CHECKIN_LIMIT:  # Check-in (Tr∆∞·ªõc 09:00)
        late_minutes = max(0, (hour - CHECKIN_START) * 60 + minute)
        collection_checkin.insert_one({
            "name": name,
            "confidence": confidence,
            "timestamp": timestamp,
            "late_minutes": late_minutes
        })
        print(f"‚úÖ Check-in: {name} - ƒêi mu·ªôn {late_minutes} ph√∫t")

    elif hour >= CHECKOUT_HOUR:  # Check-out (Sau 17:00)
        collection_checkout.insert_one({
            "name": name,
            "confidence": confidence,
            "timestamp": timestamp
        })
        print(f"‚úÖ Check-out: {name}")

    else:  # Kh√¥ng th·ªÉ ch·∫•m c√¥ng ngo√†i th·ªùi gian quy ƒë·ªãnh
        print(f"üö´ {name} kh√¥ng th·ªÉ ch·∫•m c√¥ng v√†o th·ªùi gian n√†y ({timestamp})")

# üé≠ Nh·∫≠n di·ªán khu√¥n m·∫∑t
def recognize_face(frame):
    embeddings, names = load_embeddings_from_db()
    if embeddings is None:
        return

    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    face = mtcnn(image)

    if face is None:
        print("‚ùå Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t! Vui l√≤ng th·ª≠ l·∫°i.")
        return

    face_tensor = trans(face).unsqueeze(0).to(device)

    with torch.no_grad():
        new_embedding = facenet(face_tensor).cpu().numpy().flatten()

    similarities = [1 - cosine(new_embedding, emb.flatten()) for emb in embeddings]
    best_match = np.argmax(similarities)

    if similarities[best_match] > 0.5:  # Ng∆∞·ª°ng nh·∫≠n di·ªán ch√≠nh x√°c h∆°n
        name = names[best_match]
        confidence = similarities[best_match]
        print(f"‚úÖ Nh·∫≠n di·ªán th√†nh c√¥ng: {name} ({confidence:.2f})")
        save_attendance(name, confidence)
    else:
        print("‚ùå Kh√¥ng t√¨m th·∫•y nh√¢n vi√™n trong h·ªá th·ªëng.")

# üé• M·ªü camera v√† ch·∫•m c√¥ng
def capture_and_recognize():
    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    print("üì∏ Nh√¨n v√†o camera ƒë·ªÉ ch·∫•m c√¥ng. Nh·∫•n 'SPACE' ƒë·ªÉ ch·ª•p ·∫£nh, 'ESC' ƒë·ªÉ tho√°t.")

    while True:
        ret, frame = cap.read()
        if not ret:
            print("‚ö†Ô∏è L·ªói camera! Ki·ªÉm tra l·∫°i k·∫øt n·ªëi.")
            break

        boxes, _ = mtcnn.detect(frame)
        if boxes is not None:
            for box in boxes:
                x1, y1, x2, y2 = map(int, box)
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

        cv2.imshow("Face Recognition", frame)
        key = cv2.waitKey(1) & 0xFF

        if key == 32:  # Space
            print("üïµÔ∏è‚Äç‚ôÇÔ∏è ƒêang nh·∫≠n di·ªán...")
            recognize_face(frame)

        elif key == 27:  # ESC
            break

    cap.release()
    cv2.destroyAllWindows()

# üöÄ Ch·∫°y ch∆∞∆°ng tr√¨nh
if __name__ == "__main__":
    capture_and_recognize()
