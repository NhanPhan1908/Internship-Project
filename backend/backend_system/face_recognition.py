import cv2
import torch
import numpy as np
from facenet_pytorch import InceptionResnetV1, MTCNN
from torchvision import transforms
from scipy.spatial.distance import cosine
from PIL import Image
import os
import pymongo
from datetime import datetime


DATA_PATH = './data'
os.makedirs(DATA_PATH, exist_ok=True)

# connecet database
client = pymongo.MongoClient("mongodb://localhost:27017/")
db = client["Intern"]
collection_employees = db["employees"]  # Collection ch·ª©a embedding
collection_attendance = db["attendance"]  # Collection ch·ª©a d·ªØ li·ªáu ch·∫•m c√¥ng

# check GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Init model
mtcnn = MTCNN(keep_all=False, device=device)
facenet = InceptionResnetV1(pretrained="casia-webface").to(device).eval()

# Model Init
def fixed_image_standardization(image_tensor):
    return (image_tensor - 127.5) / 128.0

def trans(img):
    """ Chuy·ªÉn ƒë·ªïi ·∫£nh th√†nh tensor n·∫øu ch∆∞a ph·∫£i """
    if isinstance(img, torch.Tensor):
        return img  # N·∫øu ƒë√£ l√† tensor, kh√¥ng c·∫ßn chuy·ªÉn ƒë·ªïi
    transform = transforms.Compose([
        transforms.ToTensor(),
        fixed_image_standardization
    ])
    return transform(img)

# List employees embeddings t·ª´ MongoDB (collection employees)
def load_embeddings_from_db():
    embeddings = []
    names = []

    for record in collection_employees.find({}, {"name": 1, "embedding": 1, "_id": 0}):
        names.append(record["name"])
        embeddings.append(np.array(record["embedding"]))  # Chuy·ªÉn sang NumPy array lu√¥n

    if len(embeddings) == 0:
        return None, None  # Tr·∫£ v·ªÅ None n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu

    return np.array(embeddings), names  # Tr·∫£ v·ªÅ NumPy array

# Save attendance v√†o MongoDB (collection attendance)
def save_to_attendance(name, confidence):
    data = {
        "name": name,
        "confidence": confidence,
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    collection_attendance.insert_one(data)
    print(f"‚úÖ ƒê√£ l∆∞u v√†o attendance: {data}")

# Capture and reconize
def capture_and_recognize():
    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    print("H√£y nh√¨n v√†o camera. Nh·∫•n 'SPACE' ƒë·ªÉ ch·ª•p ·∫£nh, 'ESC' ƒë·ªÉ tho√°t.")

    while True:
        ret, frame = cap.read()
        if not ret:
            print("Camera l·ªói!")
            break

        boxes, _ = mtcnn.detect(frame)
        if boxes is not None:
            for box in boxes:
                x1, y1, x2, y2 = map(int, box)
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2) # V·∫Ω khung
                cv2.putText(frame, "Detected", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                # ƒê√°nh d·∫•u "Detected"

        cv2.imshow("Capture Face", frame) #hien thi camera
        key = cv2.waitKey(1) & 0xFF

        if key == 32:  # Space
            print("ƒêang nh·∫≠n di·ªán...")
            recognize_face(frame)

        elif key == 27:  # ESC
            break

    cap.release()
    cv2.destroyAllWindows()


def recognize_face(frame):
    embeddings, names = load_embeddings_from_db() ## L·∫•y danh s√°ch embedding t·ª´ MongoDB

    if embeddings is None:
        print(" Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu nh·∫≠n di·ªán trong MongoDB!") 
        return

    # Chuy·ªÉn frame th√†nh ·∫£nh PIL
    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) ## Chuy·ªÉn ƒë·ªïi t·ª´ BGR (OpenCV) sang RGB (PIL)
    face = mtcnn(image)

    if face is None:
        print(" Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t! Vui l√≤ng th·ª≠ l·∫°i.")
        return  # Y√™u c·∫ßu qu√©t l·∫°i thay v√¨ ti·∫øp t·ª•c x·ª≠ l√Ω

    # Chuy·ªÉn tensor th√†nh tensor chu·∫©n h√≥a
    face_tensor = trans(face).unsqueeze(0).to(device) # Th√™m batch dimension

    # T√≠nh embedding m·ªõi
    with torch.no_grad():
        new_embedding = facenet(face_tensor).cpu().numpy().flatten()  # Flatten th√†nh vector 1D

    # cosine simirarities
    similarities = [1 - cosine(new_embedding, emb.flatten()) for emb in embeddings]  # Flatten embeddings c≈©
    best_match = np.argmax(similarities)

    # X√°c ƒë·ªãnh danh t√≠nh
    if similarities[best_match] > 0.3:
        name = names[best_match]
        confidence = similarities[best_match]
        print(f"‚úÖ Nh·∫≠n di·ªán th√†nh c√¥ng: {name} ({confidence:.2f})")
        color = (0, 255, 0)

        # L∆∞u v√†o database n·∫øu nh·∫≠n di·ªán th√†nh c√¥ng
        save_to_attendance(name, confidence)

    else:
        print(" Kh√¥ng t√¨m th·∫•y nh√¢n vi√™n! Vui l√≤ng th·ª≠ l·∫°i.")
        return  # Kh√¥ng l∆∞u v√†o database v√† y√™u c·∫ßu qu√©t l·∫°i

    # üìå Hi·ªÉn th·ªã bounding box v·ªõi t√™n nh√¢n vi√™n
    boxes, _ = mtcnn.detect(frame)
    if boxes is not None:
        for box in boxes:
            x1, y1, x2, y2 = map(int, box)
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

    cv2.imshow("Recognition Result", frame)
    cv2.waitKey(2000)


# üìå Ch·∫°y ch∆∞∆°ng tr√¨nh
if __name__ == "__main__":
    capture_and_recognize()
