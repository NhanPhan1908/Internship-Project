import cv2
import torch
import pymongo
import numpy as np
from PyQt6.QtWidgets import QApplication, QMessageBox
from PyQt6.QtGui import QImage, QPixmap
from facenet_pytorch import InceptionResnetV1, MTCNN
from torchvision import transforms
from PIL import Image
import sys
from ui_employee_register import EmployeeRegisterUI  # Import UI

# üéØ K·∫øt n·ªëi MongoDB
client = pymongo.MongoClient("mongodb://localhost:27017/")
db = client["face_recognition"]
collection = db["employees"]

# üñ• Ki·ªÉm tra GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# üîç Kh·ªüi t·∫°o model nh·∫≠n di·ªán khu√¥n m·∫∑t
mtcnn = MTCNN(keep_all=False, device=device)
facenet = InceptionResnetV1(pretrained="casia-webface").to(device).eval()

# üìå Chu·∫©n h√≥a ·∫£nh
def fixed_image_standardization(image_tensor):
    return (image_tensor - 127.5) / 128.0

def trans(img):
    transform = transforms.Compose([
        transforms.ToTensor(),
        fixed_image_standardization
    ])
    return transform(img)

class EmployeeRegister(EmployeeRegisterUI):
    def __init__(self):
        super().__init__()

        # üé• Kh·ªüi ƒë·ªông camera
        self.cap = cv2.VideoCapture(0)
        self.timer_id = self.startTimer(30)  # update camera 30ms/frame
        self.capture_button.clicked.connect(self.capture_face)  # connect button

    def timerEvent(self, event):
        """ C·∫≠p nh·∫≠t camera l√™n UI """
        ret, frame = self.cap.read()
        if ret:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # convert color
            boxes, _ = mtcnn.detect(frame) # detect face
            if boxes is not None:
                for box in boxes:
                    x1, y1, x2, y2 = map(int, box) # box = [x1, y1, x2, y2]
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2) # draw rectangle

            h, w, ch = frame.shape
            qimg = QImage(frame.data, w, h, ch * w, QImage.Format.Format_RGB888) ## convert to QImage
            self.camera_label.setPixmap(QPixmap.fromImage(qimg)) # show camera

    def capture_face(self):
        """ Ch·ª•p ·∫£nh, tr√≠ch xu·∫•t embedding & l∆∞u v√†o database """
        ret, frame = self.cap.read()
        if not ret:
            QMessageBox.critical(self, "L·ªói", "Kh√¥ng th·ªÉ ch·ª•p ·∫£nh t·ª´ camera!")
            return

        name = self.input_name.text().strip() # get name
        emp_id = self.input_id.text().strip() # get ID

        if not name or not emp_id:
            QMessageBox.warning(self, "C·∫£nh b√°o", "Vui l√≤ng nh·∫≠p ƒë·∫ßy ƒë·ªß th√¥ng tin!")
            return

        #  Recognize face
        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        face = mtcnn(image)

        if face is None:
            QMessageBox.warning(self, "L·ªói", "Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t!")
            return

        # embedding 
        with torch.no_grad():
            embedding = facenet(trans(face).unsqueeze(0).to(device)).cpu().numpy()

        # check ID
        if collection.find_one({"employee_id": emp_id}):
            QMessageBox.warning(self, "L·ªói", "M√£ nh√¢n vi√™n ƒë√£ t·ªìn t·∫°i!")
            return

        # üìå L∆∞u v√†o database
        collection.insert_one({
            "employee_id": emp_id,
            "name": name,
            "embedding": embedding.tolist()
        })

        QMessageBox.information(self, "Th√†nh c√¥ng", f"ƒê√£ ƒëƒÉng k√Ω nh√¢n vi√™n: {name}!")
        self.input_name.clear()
        self.input_id.clear()

    def closeEvent(self, event):
        """ ƒê√≥ng camera khi tho√°t ·ª©ng d·ª•ng """
        self.cap.release()
        event.accept()

# üìå Ch·∫°y ·ª©ng d·ª•ng
if __name__ == "__main__":
    app = QApplication(sys.argv)
    window = EmployeeRegister()
    window.show()
    sys.exit(app.exec())
