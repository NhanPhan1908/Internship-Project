import sys
import cv2
import requests
from PyQt6.QtWidgets import (
    QApplication, QWidget, QLabel, QVBoxLayout, QHBoxLayout,
    QPushButton, QTextEdit, QGroupBox, QTabWidget
)
from PyQt6.QtCore import Qt, QTimer, QTime
from PyQt6.QtGui import QPixmap, QImage
from PyQt6.QtWidgets import QLayout
from .camera_thread import CameraThread

BACKEND_URL = "http://127.0.0.1:8000/recognize/"

class FaceRecognitionTab(QWidget):
    def __init__(self):
        super().__init__()
        self.capture_thread = None
        self.capture = None
        self.last_frame = None
        self.prev_face = None
        self.stable_start_time = None
        self.stability_duration = 2000  # ms
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

        self.initUI()

    def initUI(self):
        self.setWindowTitle("Nháº­n diá»‡n khuÃ´n máº·t")
        self.setGeometry(100, 100, 900, 600)

        # UI components
        self.camera_viewfinder = self.create_display_label("Camera Feed")
        self.recognition_label = self.create_display_label("Recognition Result")
        self.detect_label = self.create_display_label("Detect Result")
        self.employee_data_label = self.create_display_label("Employee Data")
        self.log_output = QTextEdit()
        self.log_output.setReadOnly(True)
        self.log_output.setFixedHeight(120)

        # Buttons
        self.start_camera_button = QPushButton("Báº­t Camera")
        self.start_camera_button.clicked.connect(self.start_camera)

        self.capture_button = QPushButton("Chá»¥p & Nháº­n diá»‡n")
        self.capture_button.clicked.connect(self.capture_and_recognize_face)
        self.capture_button.setEnabled(False)

        # Layouts
        grid_layout = QVBoxLayout()
        top_row = QHBoxLayout()
        bottom_row = QHBoxLayout()

        top_row.addWidget(self.wrap_group("Camera Feed", self.camera_viewfinder))
        top_row.addWidget(self.wrap_group("Káº¿t quáº£ nháº­n diá»‡n", self.recognition_label))
        bottom_row.addWidget(self.wrap_group("Detect Result", self.detect_label))
        bottom_row.addWidget(self.wrap_group("Employee Data", self.employee_data_label))
        grid_layout.addLayout(top_row)
        grid_layout.addLayout(bottom_row)

        button_layout = QVBoxLayout()
        button_layout.addWidget(self.start_camera_button)
        button_layout.addWidget(self.capture_button)

        right_layout = QVBoxLayout()
        right_layout.addWidget(self.wrap_group("Chá»©c nÄƒng", button_layout))
        right_layout.addWidget(self.wrap_group("Pháº£n há»“i há»‡ thá»‘ng", self.log_output))

        main_layout = QHBoxLayout()
        main_layout.addLayout(grid_layout, 3)
        main_layout.addLayout(right_layout, 1)

        self.setLayout(main_layout)

    def create_display_label(self, text):
        label = QLabel(text)
        label.setAlignment(Qt.AlignmentFlag.AlignCenter)
        label.setFixedSize(480, 360)
        label.setStyleSheet("border: 1px solid black; background-color: lightgray")
        return label

    def wrap_group(self, title, content):
        group = QGroupBox(title)
        layout = QVBoxLayout()
        if isinstance(content, QLayout):
            layout.addLayout(content)
        else:
            layout.addWidget(content)
        group.setLayout(layout)
        return group

    def showEvent(self, event):
        super().showEvent(event)
        self.log_output.append("Äang má»Ÿ camera...")
        self.start_camera()

    def start_camera(self):
        if self.capture_thread and self.capture_thread.isRunning():
            self.log_output.append("ğŸ“· Camera Ä‘Ã£ báº­t.")
            return

        self.capture_thread = CameraThread()
        self.capture = self.capture_thread.capture
        self.capture_thread.frame_ready.connect(self.update_frame)
        self.capture_thread.running = True
        self.capture_thread.start()
        self.capture_button.setEnabled(True)
        self.log_output.append("âœ… Camera khá»Ÿi Ä‘á»™ng thÃ nh cÃ´ng.")

    def stop_camera(self):
        if self.capture_thread and self.capture_thread.isRunning():
            self.capture_thread.stop()
            self.capture_thread = None
            self.log_output.append("ğŸ›‘ Camera Ä‘Ã£ dá»«ng.")

    def closeEvent(self, event):
        self.stop_camera()
        event.accept()

    def update_frame(self, frame):
        if frame is None:
            return

        self.last_frame = frame.copy()
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)

        if len(faces) > 0:
            (x, y, w, h) = faces[0]
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

            if self.prev_face:
                dx, dy, dw, dh = [abs(a - b) for a, b in zip((x, y, w, h), self.prev_face)]
                if dx < 40 and dy < 40 and dw < 40 and dh < 40:
                    if not self.stable_start_time:
                        self.stable_start_time = QTime.currentTime()
                    else:
                        elapsed = self.stable_start_time.msecsTo(QTime.currentTime())
                        if elapsed > self.stability_duration:
                            self.log_output.append("âœ… GÆ°Æ¡ng máº·t á»•n Ä‘á»‹nh - tiáº¿n hÃ nh chá»¥p...")
                            self.capture_and_recognize_face()
                            self.stable_start_time = None
                else:
                    self.stable_start_time = None
            else:
                self.stable_start_time = QTime.currentTime()
            self.prev_face = (x, y, w, h)
        else:
            self.prev_face = None
            self.stable_start_time = None

        self.display_image(frame, self.camera_viewfinder)

    def capture_and_recognize_face(self):
        if self.capture is None:
            self.log_output.append("âŒ Camera chÆ°a Ä‘Æ°á»£c má»Ÿ.")
            return
        if self.last_frame is None:
            self.log_output.append("âŒ KhÃ´ng cÃ³ áº£nh Ä‘á»ƒ nháº­n diá»‡n.")
            return

        ret, frame = self.capture.read()
        if not ret:
            self.log_output.append("âŒ KhÃ´ng thá»ƒ chá»¥p áº£nh tá»« camera.")
            return

        self.log_output.append("ğŸ”„ Äang gá»­i áº£nh Ä‘áº¿n backend...")
        _, img_encoded = cv2.imencode(".jpg", frame)
        files = {"file": ("face.jpg", img_encoded.tobytes(), "image/jpeg")}

        try:
            response = requests.post(BACKEND_URL, files=files)
            if response.status_code == 200:
                data = response.json()
                self.display_recognition_result(data)
            else:
                self.log_output.append(f"âŒ Lá»—i server: {response.status_code}")
        except Exception as e:
            self.log_output.append(f"âŒ Lá»—i gá»­i áº£nh: {repr(e)}")

    def display_recognition_result(self, result):
        self.log_output.append("ğŸ“¸ Káº¿t quáº£ nháº­n diá»‡n:")
        self.log_output.append(f"ğŸ”¹ Tráº¡ng thÃ¡i: {result.get('status', 'unknown')}")
        self.log_output.append(f"ğŸ”¹ PhÃ¡t hiá»‡n: {'âœ… CÃ³' if result.get('detect') else 'âŒ KhÃ´ng'}")
        self.log_output.append(f"ğŸ”¹ Nháº­n diá»‡n: {'âœ… CÃ³' if result.get('recognize') else 'âŒ KhÃ´ng'}")
        self.log_output.append(f"ğŸ”¹ TÃªn: {result.get('name', 'KhÃ´ng rÃµ')}")
        self.log_output.append(f" employee_id: {result.get('employee_id', 'N/A')}")
        self.log_output.append(f"ğŸ”¹ Äá»™ tá»± tin: {result.get('confidence', 'N/A')}")
        self.log_output.append(f"ğŸ’¬ {result.get('message', '')}")

    def display_image(self, img, label):
        rgb_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        height, width, channel = rgb_image.shape
        bytes_per_line = 3 * width
        qt_image = QImage(rgb_image.data, width, height, bytes_per_line, QImage.Format.Format_RGB888)
        label.setPixmap(QPixmap.fromImage(qt_image).scaled(label.width(), label.height(), Qt.AspectRatioMode.KeepAspectRatio))
